{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import fmin, hp, tpe, Trials\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "BASEDIR = os.path.dirname(os.getcwd())\n",
    "sys.path.append(BASEDIR)\n",
    "\n",
    "from copy import deepcopy\n",
    "from functools import partial\n",
    "from dgl.dataloading import GraphDataLoader\n",
    "from src.model.attentivefp_gp import attentivefpGP\n",
    "from src.utils.mol.attfp_graph import MoleculeDataset, collate_fn\n",
    "from src.config.attentivefp_gp import attentivefpGPArgs\n",
    "from src.pipeline.ensemble import training_ensemble_models\n",
    "from src.utils.basic.logger import Writer, NpEncoder\n",
    "from src.utils.model.metrics import accuracy, roc_auc_score, prc_auc, F1, MCC, expected_calibration_error, OverconfidentFalseRate, OverconfidentFalseNegatives, Brier\n",
    "import torch\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_num_threads(4)\n",
    "\n",
    "target_list = ['ames', 'BBB_Martins', 'Pgp_Broccatelli', 'CYP3A4_Veith', 'CYP2C9_Veith']\n",
    "gpu_num     = 0\n",
    "\n",
    "# Bayesian optimization searching space\n",
    "\n",
    "SPACE = {\n",
    "    'hidden_size'       : hp.quniform('hidden_size', low=300, high=600, q=100),\n",
    "    'radius'            : hp.quniform('radius', low=2, high=6, q=1),\n",
    "    'T'                 : hp.quniform('T', low=1, high=5, q=1),\n",
    "    'p_dropout'         : hp.quniform('dropout', low=0.0, high=0.5, q=0.05),\n",
    "    'init_lr'           : hp.loguniform('init_lr', low=np.log(1e-4), high=np.log(1e-2)),\n",
    "    'ffn_num_layers'    : hp.quniform('ffn_num_layers', low=2, high=4, q=1),\n",
    "    'n_inducing_points' : hp.quniform('n_inducing_points', low=100, high=300, q=50)\n",
    "}\n",
    "\n",
    "INT_KEYS = ['hidden_size', 'radius', 'T', 'ffn_num_layers', 'n_inducing_points']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for target_name in target_list:\n",
    "    \n",
    "    SAVEDIR = os.path.join(BASEDIR, \"results\", \"TDC\", target_name, 'AttFpGP')\n",
    "    DATADIR = os.path.join(BASEDIR, \"data\", target_name)\n",
    "    n = 0\n",
    "    logger = Writer(os.path.join(SAVEDIR, \"history.log\"))\n",
    "    def func(hyperparams):\n",
    "        logger(\" \")\n",
    "        logger(\" \")\n",
    "\n",
    "        global n\n",
    "        n = n+1\n",
    "        logger(f\"ROUND {n}\")\n",
    "\n",
    "        BASESAVEDIR = os.path.join(SAVEDIR, f\"ROUND_{n}\")\n",
    "        logger_writer = Writer(os.path.join(BASESAVEDIR, \"config.txt\"))\n",
    "        for k in hyperparams:\n",
    "            logger_writer(f\"{k}\\t{hyperparams[k]}\")\n",
    "\n",
    "        for key in INT_KEYS:\n",
    "            hyperparams[key] = int(hyperparams[key])\n",
    "\n",
    "        config = attentivefpGPArgs().parse_args([], known_only=True)\n",
    "        hyper_args = deepcopy(config)\n",
    "\n",
    "        for key, value in hyperparams.items():\n",
    "            setattr(hyper_args, key, value)\n",
    "\n",
    "        setattr(hyper_args, \"dataset_type\", \"classification\")\n",
    "        setattr(hyper_args, 'metric', \"roc-auc\")\n",
    "        setattr(hyper_args, \"extra_metrics\", [\"MCC\", \"prc-auc\", \"accuracy\", \"F1\"])\n",
    "        setattr(hyper_args, \"ffn_hidden_size\", hyper_args.hidden_size)\n",
    "        setattr(hyper_args, \"early_stopping_num\", 30)\n",
    "        setattr(hyper_args, \"gpu\", gpu_num)\n",
    "        setattr(hyper_args, \"log_frequency\", 100)\n",
    "        setattr(hyper_args, \"batch_size\", 128)\n",
    "        setattr(hyper_args, \"at_least_epoch\", 0)\n",
    "        print(hyper_args)\n",
    "\n",
    "        for i in [1, 2, 3, 4, 5]:\n",
    "\n",
    "            train_dataset = MoleculeDataset(os.path.join(DATADIR, f\"{target_name}_train_{i}.csv\"))\n",
    "            valid_dataset = MoleculeDataset(os.path.join(DATADIR, f\"{target_name}_valid_{i}.csv\"))\n",
    "            test_dataset  = MoleculeDataset(os.path.join(DATADIR, f\"{target_name}_test.csv\"))\n",
    "\n",
    "            train_targets = []\n",
    "            for _, t in train_dataset:\n",
    "                train_targets.append(t)\n",
    "\n",
    "            N = torch.tensor([len(train_targets) - np.sum(train_targets).astype(np.int64),\n",
    "                            np.sum(train_targets).astype(np.int64)], dtype=torch.float64)\n",
    "\n",
    "            setattr(hyper_args, \"N\", N)\n",
    "            \n",
    "            train_dataloader = GraphDataLoader(dataset=train_dataset, collate_fn=collate_fn, batch_size=256, drop_last=False, shuffle=True)\n",
    "            train_dataloader.smiles = [[s] for s in train_dataset.smiles_list]\n",
    "            valid_dataloader = GraphDataLoader(dataset=valid_dataset, collate_fn=collate_fn, batch_size=256, drop_last=False, shuffle=False)\n",
    "            valid_dataloader.smiles = [[s] for s in valid_dataset.smiles_list]\n",
    "            test_dataloader  = GraphDataLoader(dataset=test_dataset,  collate_fn=collate_fn, batch_size=256, drop_last=False, shuffle=False)\n",
    "            test_dataloader.smiles  = [[s] for s in test_dataset.smiles_list]\n",
    "\n",
    "            training_ensemble_models(os.path.join(BASESAVEDIR, f\"fold_{i}\"),\n",
    "                                    attentivefpGP,\n",
    "                                    hyper_args,\n",
    "                                    train_dataloader,\n",
    "                                    valid_dataloader=valid_dataloader,\n",
    "                                    test_dataloader=test_dataloader,\n",
    "                                    ensemble_num=1)\n",
    "            \n",
    "            test_prediction = []\n",
    "            \n",
    "            valid_ROC = []\n",
    "            valid_PRC = []\n",
    "            valid_ACC = []\n",
    "            valid_MCC = []\n",
    "            valid_F1  = []\n",
    "            valid_ECE = []\n",
    "            valid_OFR = []\n",
    "            valid_OFN = []\n",
    "            valid_Brier = []\n",
    "            \n",
    "            test_ROC  = []\n",
    "            test_PRC  = []\n",
    "            test_ACC  = []\n",
    "            test_MCC  = []\n",
    "            test_F1   = []\n",
    "            test_ECE  = []\n",
    "            test_OFR = []\n",
    "            test_OFN = []\n",
    "            test_Brier = []\n",
    "\n",
    "        for i in [1, 2, 3, 4, 5]:\n",
    "        \n",
    "            temp_dir = os.path.join(BASESAVEDIR, f\"fold_{i}\", \"model_0\")\n",
    "            \n",
    "            temp_valid_prediction = pd.read_csv(os.path.join(temp_dir, \"valid_prediction.csv\"))[\"property_pred\"].to_numpy()\n",
    "            temp_valid_label = pd.read_csv(os.path.join(temp_dir, \"valid_prediction.csv\"))[\"property_label\"].to_numpy()\n",
    "            \n",
    "            temp_test_prediction = pd.read_csv(os.path.join(temp_dir, \"test_prediction.csv\"))[\"property_pred\"].to_numpy()\n",
    "            test_label = pd.read_csv(os.path.join(temp_dir, \"test_prediction.csv\"))[\"property_label\"].to_numpy()\n",
    "            test_prediction.append(temp_test_prediction) # for ensemble calculation\n",
    "            \n",
    "            valid_ROC.append(pd.read_csv(os.path.join(temp_dir, \"valid_prediction_performance.csv\"))[\"roc-auc\"].iloc[0])\n",
    "            valid_PRC.append(pd.read_csv(os.path.join(temp_dir, \"valid_prediction_performance.csv\"))[\"prc-auc\"].iloc[0])\n",
    "            valid_ACC.append(pd.read_csv(os.path.join(temp_dir, \"valid_prediction_performance.csv\"))[\"accuracy\"].iloc[0])\n",
    "            valid_MCC.append(pd.read_csv(os.path.join(temp_dir, \"valid_prediction_performance.csv\"))[\"MCC\"].iloc[0])\n",
    "            valid_F1.append(pd.read_csv(os.path.join(temp_dir, \"valid_prediction_performance.csv\"))[\"F1\"].iloc[0])\n",
    "            valid_ECE.append(expected_calibration_error(temp_valid_label, temp_valid_prediction, bins=10))\n",
    "            valid_OFR.append(OverconfidentFalseRate(temp_valid_prediction, temp_valid_label))\n",
    "            valid_OFN.append(OverconfidentFalseNegatives(temp_valid_prediction, temp_valid_label))\n",
    "            valid_Brier.append(Brier(temp_valid_label,temp_valid_prediction))\n",
    "            \n",
    "            test_ROC.append(pd.read_csv(os.path.join(temp_dir, \"test_prediction_performance.csv\"))[\"roc-auc\"].iloc[0])\n",
    "            test_PRC.append(pd.read_csv(os.path.join(temp_dir, \"test_prediction_performance.csv\"))[\"prc-auc\"].iloc[0])\n",
    "            test_ACC.append(pd.read_csv(os.path.join(temp_dir, \"test_prediction_performance.csv\"))[\"accuracy\"].iloc[0])\n",
    "            test_MCC.append(pd.read_csv(os.path.join(temp_dir, \"test_prediction_performance.csv\"))[\"MCC\"].iloc[0])\n",
    "            test_F1.append(pd.read_csv(os.path.join(temp_dir, \"test_prediction_performance.csv\"))[\"F1\"].iloc[0])\n",
    "            test_ECE.append(expected_calibration_error(test_label, temp_test_prediction, bins=10))\n",
    "            test_OFR.append(OverconfidentFalseRate(temp_test_prediction, test_label))\n",
    "            test_OFN.append(OverconfidentFalseNegatives(temp_test_prediction, test_label))\n",
    "            test_Brier.append(Brier(test_label,temp_test_prediction))\n",
    "            \n",
    "\n",
    "        logger(f'ROUND {n} Valid ROC-AUC {np.mean(valid_ROC)} +/- {np.std(valid_ROC)}')\n",
    "        logger(f'ROUND {n} Valid PRC-AUC {np.mean(valid_PRC)} +/- {np.std(valid_PRC)}')\n",
    "        logger(f'ROUND {n} Valid ACC     {np.mean(valid_ACC)} +/- {np.std(valid_ACC)}')\n",
    "        logger(f'ROUND {n} Valid MCC     {np.mean(valid_MCC)} +/- {np.std(valid_MCC)}')\n",
    "        logger(f'ROUND {n} Valid F1      {np.mean(valid_F1)} +/- {np.std(valid_F1)}')\n",
    "        logger(f'ROUND {n} Valid ECE     {np.mean(valid_ECE)} +/- {np.std(valid_ECE)}')\n",
    "        logger(f'ROUND {n} Valid OFR     {np.mean(valid_OFR)} +/- {np.std(valid_OFR)}')\n",
    "        logger(f'ROUND {n} Valid OFN     {np.mean(valid_OFN)} +/- {np.std(valid_OFN)}')\n",
    "        logger(f'ROUND {n} Valid Brier   {np.mean(valid_Brier)} +/- {np.std(valid_Brier)}')\n",
    "        logger(' ')\n",
    "        logger(f'ROUND {n} Test ROC-AUC {np.mean(test_ROC)} +/- {np.std(test_ROC)}')\n",
    "        logger(f'ROUND {n} Test PRC-AUC {np.mean(test_PRC)} +/- {np.std(test_PRC)}')\n",
    "        logger(f'ROUND {n} Test ACC     {np.mean(test_ACC)} +/- {np.std(test_ACC)}')\n",
    "        logger(f'ROUND {n} Test MCC     {np.mean(test_MCC)} +/- {np.std(test_MCC)}')\n",
    "        logger(f'ROUND {n} Test F1      {np.mean(test_F1)} +/- {np.std(test_F1)}')\n",
    "        logger(f'ROUND {n} Test ECE     {np.mean(test_ECE)} +/- {np.std(test_ECE)}')\n",
    "        logger(f'ROUND {n} Test OFR     {np.mean(test_OFR)} +/- {np.std(test_OFR)}')\n",
    "        logger(f'ROUND {n} Test OFN     {np.mean(test_OFN)} +/- {np.std(test_OFN)}')\n",
    "        logger(f'ROUND {n} Test Brier   {np.mean(test_Brier)} +/- {np.std(test_Brier)}')\n",
    "        logger(' ')\n",
    "        logger(f'ROUND {n} Ensemble Test ROC-AUC {roc_auc_score(test_label, np.mean(test_prediction, axis=0))}')\n",
    "        logger(f'ROUND {n} Ensemble Test PRC-AUC {prc_auc(test_label, np.mean(test_prediction, axis=0))}')\n",
    "        logger(f'ROUND {n} Ensemble Test ACC {accuracy(test_label, np.mean(test_prediction, axis=0))}')\n",
    "        logger(f'ROUND {n} Ensemble Test MCC {MCC(test_label, np.mean(test_prediction, axis=0))}')\n",
    "        logger(f'ROUND {n} Ensemble Test F1 {F1(test_label, np.mean(test_prediction, axis=0))}')\n",
    "        logger(f'ROUND {n} Ensemble Test ECE {expected_calibration_error(test_label, np.mean(test_prediction, axis=0), bins=10)}')\n",
    "        logger(f'ROUND {n} Ensemble Test OFR {OverconfidentFalseRate(np.mean(test_prediction, axis=0), test_label)}')\n",
    "        logger(f'ROUND {n} Ensemble Test OFN {OverconfidentFalseNegatives(np.mean(test_prediction, axis=0), test_label)}')\n",
    "        logger(f'ROUND {n} Ensemble Test Brier {Brier(test_label, np.mean(test_prediction, axis=0))}')\n",
    "        logger(' ')\n",
    "\n",
    "        return -np.mean(valid_ROC)\n",
    "\n",
    "\n",
    "\n",
    "    algo = partial(tpe.suggest, n_startup_jobs=1)\n",
    "    best = fmin(func, SPACE, algo=algo, max_evals=1)  # best is a dictionary\n",
    "    json_str = json.dumps(best, cls=NpEncoder)  # using json to turn a dictionary to a str \n",
    "    logger(json_str)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "postnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
